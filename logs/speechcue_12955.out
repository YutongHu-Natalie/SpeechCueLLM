Job started at Thu Jan 22 14:44:28 EST 2026
Running on host: tebuna
Job ID: 12955
SLURM allocated GPUs: 0
Thu Jan 22 14:44:28 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A6000               On  |   00000000:B2:00.0 Off |                  Off |
| 30%   28C    P8             16W /  300W |       2MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A6000               On  |   00000000:B5:00.0 Off |                  Off |
| 30%   32C    P8             26W /  300W |       2MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
******************************************************************************************
All parameters are valid.
The dataset you have selected is: iemocap !
The base model you have selected is OpenAI-gpt-5-mini!
The model's SFT method you have selected: zero_shot!
If predict emotions: False
******************************************************************************************
******************************************************************************************
Data procession has executed successfully !
******************************************************************************************
******************************************************************************************
Your choose iemocap! The max_context_length will be set as 2500!
******************************************************************************************
Your choose is not in MY candidations! Please check your Model name!
Your choose OpenAI-gpt-5-mini! Model Parameters should be initialized in the path 
 
Your choose zero_shot! The experiment will be set as ZERO_SHOT model
******************************************************************************************
Using OpenAI API with model: gpt-5-mini
Processed Data_Path: ../PROCESSED_DATASET/iemocap/window/True_False
******************************************************************************************
Loaded 1623 test samples
Using zero-shot prompting

***** Running inference with gpt-5-mini (zero_shot) *****

API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 2): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}
API call failed (attempt 1): Error code: 400 - {'error': {'message': 'Could not finish the message because max_tokens or model output limit was reached. Please try again with higher max_tokens.', 'type': 'invalid_request_error', 'param': None, 'code': None}}

***** Results *****
Accuracy: 15.65%
Weighted F1: 5.081%

Classification Report:
              precision    recall  f1-score   support

         hap   1.000000  0.006944  0.013793       144
         sad   0.151985  1.000000  0.263866       245
         neu   1.000000  0.007812  0.015504       384
         ang   0.000000  0.000000  0.000000       170
         exc   0.600000  0.010033  0.019737       299
         fru   1.000000  0.005249  0.010444       381

    accuracy                       0.156500      1623
   macro avg   0.625331  0.171673  0.053891      1623
weighted avg   0.693553  0.156500  0.050812      1623


Results saved to: ../experiments/OpenAI-gpt-5-mini/zero_shot/iemocap/window_12/per_1.0_des_context_class5_11/preds_for_eval.text
Job finished at Thu Jan 22 17:03:45 EST 2026
