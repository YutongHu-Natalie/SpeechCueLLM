Job started at Thu Nov 20 10:53:25 AM EST 2025
Running on host: h200server
Job ID: 20929
SLURM allocated GPUs: 0,1
Thu Nov 20 10:53:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    Off |   00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             85W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    Off |   00000000:AB:00.0 Off |                    0 |
| N/A   38C    P0            106W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
******************************************************************************************
All parameters are valid.
The dataset you have selected is: iemocap !
The base model you have selected is LLaMA3-instruct-70b!
The model's SFT method you have selected: lora!
If predict emotions: False
******************************************************************************************
******************************************************************************************
Data procession has executed successfully !
******************************************************************************************
******************************************************************************************
Your choose iemocap! The max_context_length will be set as 2500!
******************************************************************************************
Your choose LLaMA3-instruct-70b! Model Parameters should be initialized in the path 
 /local/scratch/yhu383/models/llama3.3-70b
Your choose lora! The experiment will be set as LORA model
Processed Data_Path: ../PROCESSED_DATASET/iemocap/window/True_False
[2025-11-20 10:53:44,188] [WARNING] [runner.py:191:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1: setting --include=localhost:0,1
[2025-11-20 10:53:44,235] [INFO] [runner.py:541:main] cmd = /local/scratch/yhu383/SpeechCueLLM/venv/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=26000 --enable_each_rank_log=None main.py --dataset iemocap --model_name_or_path /local/scratch/yhu383/models/llama3.3-70b --data_dir ../PROCESSED_DATASET/iemocap/window/True_False --output_dir ../experiments/LLaMA3-instruct-70b/lora/iemocap/window_12/LR_3e-4_BS_8_per_1.0_des_context_class5_11 --max_length 2500 --batch_size 8 --deepspeed_config ../LLM_code/data_utils/deepspeed_config.json --gradient_accumulation_steps 8 --eval_batch_size 8 --num_train_epochs 15 --save_steps 100000 --lora True --learning_rate 3e-4 --do_eval True --do_train True --statistic_mode True --data_percent 1.0 --seed 11 --emotion_prediction False
[2025-11-20 10:53:47,320] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-11-20 10:53:47,320] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-11-20 10:53:47,320] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-11-20 10:53:47,320] [INFO] [launch.py:247:main] dist_world_size=2
[2025-11-20 10:53:47,320] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1
ModelArgs(model_type='decoder', model_name_or_path='/local/scratch/yhu383/models/llama3.3-70b', checkpoint_dir=None, output_dir='../experiments/LLaMA3-instruct-70b/lora/iemocap/window_12/LR_3e-4_BS_8_per_1.0_des_context_class5_11', data_dir='../PROCESSED_DATASET/iemocap/window/True_False', do_train=True, do_eval=True, warmup_ratio=0.1, warmup_steps=None, save_steps=100000, weight_decay=0.0, max_seq_length=256, max_length=2500, num_beams=1, do_sample=False, top_k=None, top_p=None, learning_rate=0.0003, preprocess_inputs=True, clip_norm=1.0, open_ended=False, batch_size=8, eval_batch_size=8, gradient_accumulation_steps=8, lora=True, lora_dim=16, lora_alpha=16, lora_dropout=0.05, lora_module_name='q_proj,k_proj,v_proj,query_key_value', seed=11, offload_optimizer=False, deepspeed_config='../LLM_code/data_utils/deepspeed_config.json', zero_shot=True, mode='sft', gradient_checkpointing=False)
ModelArgs(model_type='decoder', model_name_or_path='/local/scratch/yhu383/models/llama3.3-70b', checkpoint_dir=None, output_dir='../experiments/LLaMA3-instruct-70b/lora/iemocap/window_12/LR_3e-4_BS_8_per_1.0_des_context_class5_11', data_dir='../PROCESSED_DATASET/iemocap/window/True_False', do_train=True, do_eval=True, warmup_ratio=0.1, warmup_steps=None, save_steps=100000, weight_decay=0.0, max_seq_length=256, max_length=2500, num_beams=1, do_sample=False, top_k=None, top_p=None, learning_rate=0.0003, preprocess_inputs=True, clip_norm=1.0, open_ended=False, batch_size=8, eval_batch_size=8, gradient_accumulation_steps=8, lora=True, lora_dim=16, lora_alpha=16, lora_dropout=0.05, lora_module_name='q_proj,k_proj,v_proj,query_key_value', seed=11, offload_optimizer=False, deepspeed_config='../LLM_code/data_utils/deepspeed_config.json', zero_shot=True, mode='sft', gradient_checkpointing=False)
the number of num_samples is 5163
the number of num_samples is 5163
the number of num_samples is 1623
the number of num_samples is 1623
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
ninja: no work to do.
Time to load cpu_adam op: 3.031446933746338 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-11-20 10:55:37,819] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown
the number of num_samples is 5163
the number of num_samples is 5163
the number of num_samples is 1623
the number of num_samples is 1623
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
ninja: no work to do.
Time to load cpu_adam op: 3.174509048461914 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000300, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-11-20 10:55:42,971] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown
[2025-11-20 10:55:42,971] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-11-20 10:55:45,350] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead
[2025-11-20 10:55:45,355] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead
[2025-11-20 10:55:53,462] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 3541633
[2025-11-20 10:55:53,555] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 3541634
[2025-11-20 10:55:53,555] [ERROR] [launch.py:434:sigkill_handler] ['/local/scratch/yhu383/SpeechCueLLM/venv/bin/python', '-u', 'main.py', '--local_rank=1', '--dataset', 'iemocap', '--model_name_or_path', '/local/scratch/yhu383/models/llama3.3-70b', '--data_dir', '../PROCESSED_DATASET/iemocap/window/True_False', '--output_dir', '../experiments/LLaMA3-instruct-70b/lora/iemocap/window_12/LR_3e-4_BS_8_per_1.0_des_context_class5_11', '--max_length', '2500', '--batch_size', '8', '--deepspeed_config', '../LLM_code/data_utils/deepspeed_config.json', '--gradient_accumulation_steps', '8', '--eval_batch_size', '8', '--num_train_epochs', '15', '--save_steps', '100000', '--lora', 'True', '--learning_rate', '3e-4', '--do_eval', 'True', '--do_train', 'True', '--statistic_mode', 'True', '--data_percent', '1.0', '--seed', '11', '--emotion_prediction', 'False'] exits with return code = 1
Job finished at Thu Nov 20 10:55:54 AM EST 2025
